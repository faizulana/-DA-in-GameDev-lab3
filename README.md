# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #3 выполнил(а):
- Файзулина Милана Мансуровна
- РИ000024
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity. 
Ход работы:
- Создать и активировать ML агент, установить необходимые библиотеки.
![image](mlagent_install.png)
![image](torch_install.png)

- Создать сферу, куб и плоскость в Unity, подключить к сфере c# скрипт и настроить компоненты.

![image](sphere_properties.png)

- Запустить работу ML агента. Сделать 3, 9, 27 копий модели «Плоскость-Сфера-Куб», запустить симуляцию сцены и понаблюдать за результатом обучения модели. После завершения обучения проверить работу модели и сделать выводы.
![image](learning1.png)
![image](learning9.png)
![image](learning27.png)

![rolleragent](rolleragent.gif)

Вывод: увеличение количества копий модели, выполняющих действие одновременно, помогает сделать обучение быстрее. Также было замечено, что изменение Mean Reward и Std of Reward может быть неравномерным, с временным ухудшением показателей. Возможно, для минимизации такого отклонения нужно отрегулировать параметры конфигурации нейронной сети.

## Задание 2
Подробно опишите каждую строку файла конфигурации нейронной сети, доступного в папке с файлами проекта. Самостоятельно найдите информацию о компонентах Decision Requester, Behavior Parameters, добавленных на сфере.
- trainer_type: ppo
Proximal Policy Optimization (PPO) - один из относительно недавних алгоритмов глубоко обучения с подкреплением. PPO старается делать свои новые политики более похожими на старые с помощью более простых методов чем другие алгоритмы из его семейства. Это означает, что алгоритм исследует возможные новые действия на основе уже существующей политики. То, насколько случайными будут результаты следующего шага вычислений зависит от начальных условий и процедуры обучения. Обычно, в процессе обучения политика становится менее случайной, так как правила алгоритма подразумевают, что действия, которые приносили хорошие награды в прошлом, должны продолжать использоваться далее.
- hyperparameters:
Гиперпараметр модели - это конфигурация, внешняя по отношению к модели, значение которой невозможно оценить по данным. Задаются до запуска алгоритма и используются для управления процессом обучения.
- batch_size: 10
Определяет количество иинформации, собираемой в ходе одного шага в градиентном спуске. Связан с параметром buffer_size, всегда должен быть его долей.
- buffer_size: 100
Определяет, сколько информации полученных в ходе выполнения действий будет сохранено в буфере перед тем, как на основе этих данных произойдет обучение или изменение модели. Чем выше этот показатель, тем стабильнее будет происходить обучение. 
- learning_rate: 3.0e-4
Параметр задает величину каждого шага обновления градиентного спуска. Обычно это значение следует уменьшить, если обучение нестабильно, а вознаграждение не увеличивается последовательно.
- beta: 5.0e-4
Параметр задает степень регуляризации энтропии, и определяет, какой будет степень случайности в обновленной политике. Чем выше этот параметр, тем больше случайных действий будет выполнять модель. При оптимальном подборе параметра энтропия будет уменьшаться с увеличением среднего вознаграждения. 
- epsilon: 0.2
Соответствует допустимому порогу расхождения между старой и новой политиками при обновлении с градиентным спуском. Чем меньше показатель, тем более стабильно будет происходить обновление, но тем более медленно будет проходить процесс обучения.
- lambd: 0.99
Используется при расчете Обобщенной Оценки Преимущества (GAE). Обозначает то, насколько агент полагается на текущую оценку значения при расчете обновленной ооценки значения. Чем выше значение, тем выше зависимость от фактических вознаграждений, полученных извне,что может привести к высокой дисперсии.
- num_epoch: 3
Определяет количество проходов через буфер опыта во время градиентного спуска. Чем меньше параметр, тем стабильнее происходят обновления за счет более медленного обучения.
- learning_rate_schedule: linear
Графики скорости обучения определяют, по какому графику будет проводиться корректировка скорости обучения путем снижения скорости обучения. Линейный график уменьшает скорость обучения на одну и ту же величину каждый цикл.
- network_settings:
Параметры нейронной сети
- normalize: false
К входным данным векторного наблюдения не будет применяться нормализация. Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна при более простых задачах дискретного управления.
- hidden_units: 128
Соответствует количеству единиц в каждом полностью подключенном слое нейронной сети. Для простых задач, где правильным действием является простая комбинация входных данных наблюдения, это должно быть небольшим. Для задач, где действие представляет собой очень сложное взаимодействие между переменными наблюдения, это должно быть больше.
- num_layers: 2
Определяет количество слоев нейронной сети. Для простых задач меньшее количество слоев, скорее всего, будет обучаться быстрее и эффективнее. Для более сложных задач управления может потребоваться больше уровней.
- reward_signals:
Параметры вознаграждений
- gamma: 0.99
Параметр задает степень значимости будущих вознаграждений. Чем выше значение, тем больше агент должен действовать в настоящем, чтобы подготовиться к вознаграждению в отдаленном будущем. 
- max_steps: 500000
Параметр задает количество шагов моделирования (умноженных на частоту кадров), выполняемых в процессе обучения. Чем сложнее задача, тем выше должно быть это значение.
- time_horizon: 64
Параметр соответствует количеству шагов опыта, которые необходимо собрать для каждого агента, прежде чем добавлять его в буфер опыта. Когда этот предел достигается до окончания эпизода, оценка стоимости используется для прогнозирования общего ожидаемого вознаграждения от текущего состояния агента. Это число должно быть достаточно большим, чтобы охватить все важное поведение в рамках последовательности действий агента.

- DecisionRequester запрашивает принятие решения на основе сделанных наблюдений через определенные промежутки времени. Иными словами, DecisionRequester определяет, сколько шагов Академии (глобального одноэлементного “мозга” ML-Agentа) должно быть выполнено, прежде чем будет запрошено решение. 
- BehaviorParameters определяет, сколько наблюдений мы принимаем и какую форму будут принимать выводимые действия.
- BehaviorType имеет три варианта: эвристический (heuristic), по умолчанию (default) и вывод (inference). При установке значения по умолчанию, если нейронная сеть была сгенерирована, агент будет выполнять Inference, поскольку он использует нейронную сеть для принятия решений. Когда нейронная сеть не предусмотрена, она будет использовать эвристику. Эвристический метод можно рассматривать как традиционный подход к искусственному интеллекту, при котором программист вводит все возможные команды непосредственно в объект. Если установлено значение Heuristic only, агент будет запускать все, что находится в эвристическом методе.

## Задание 3
Доработайте сцену и обучите ML-Agent таким образом, чтобы шар перемещался между двумя кубами разного цвета. Кубы должны, как и в первом задании, случайно изменять координаты на плоскости. 

Ход работы:

- Изменить c# скрипт
```c#
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;

public class RollerAgent : Agent
{
    Rigidbody rBody;
    // Start is called before the first frame update
    void Start()
    {
        rBody = GetComponent<Rigidbody>();
    }

    public GameObject Target;
    public GameObject Target1;
    private bool touch_target;
    private bool touch_target1;

    public override void OnEpisodeBegin()
    {
        if (this.transform.localPosition.y < 0)
        {
            this.rBody.angularVelocity = Vector3.zero;
            this.rBody.velocity = Vector3.zero;
            this.transform.localPosition = new Vector3(0, 0.5f, 0);
        }

        Target.transform.localPosition = new Vector3(Random.value * 8-4, 0.5f, Random.value * 8-4);
        Target1.transform.localPosition = new Vector3(Random.value * 8-4, 0.5f, Random.value * 8-4);
        Target.SetActive(true);
        Target1.SetActive(true);
        touch_target = false;
        touch_target1 = false;
    }
    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(Target.transform.localPosition);
        sensor.AddObservation(Target1.transform.localPosition);
        sensor.AddObservation(this.transform.localPosition);
        sensor.AddObservation(touch_target);
        sensor.AddObservation(touch_target1);
        sensor.AddObservation(rBody.velocity.x);
        sensor.AddObservation(rBody.velocity.z);
    }

    public float forceMultiplier = 10;
    public override void OnActionReceived(ActionBuffers actionBuffers)

    {
        Vector3 controlSignal = Vector3.zero;
        controlSignal.x = actionBuffers.ContinuousActions[0];
        controlSignal.z = actionBuffers.ContinuousActions[1];
        rBody.AddForce(controlSignal * forceMultiplier);

        float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.transform.localPosition);
        float distanceToTarget1 = Vector3.Distance(this.transform.localPosition, Target1.transform.localPosition);

        if (!touch_target & distanceToTarget < 1.42f)
        {
            touch_target = true;
            Target.SetActive(false);
        }

        if (!touch_target1 & distanceToTarget1 < 1.42f)
        {
            touch_target1 = true;
            Target1.SetActive(false);

        }

        if (touch_target & touch_target1)
        {
            SetReward(1.0f);
            EndEpisode();
        }

        else if (this.transform.localPosition.y < 0)
        {
            SetReward(-0.5f);
            EndEpisode();
        }
    }
}
```
- Обучить ML агент
![image](learning_between_two.png)
Результат:
![rolling-between-two](rolling-between-two.gif)

## Выводы
  В ходе выполонения Лабораторной работы я ознакомилась с базовым синтаксисом C#, классами, используемыми в скриптинге Unity, а также основами использования ML агента для оптимизации траектории движения интеллектуального агента.
  Как мы увидели в Лабораторной, ML агенты могут быть использованы для выбора стратегии в ситуациях, когда переменных слишком много, чтобы оптимизировать алгоритм вручную. Решение подобных задач может быть важно для рассчета игрового баланса. Игровой баланс - соблюдение равновесия между различными экономическими показателями в игре, чтобы игровой процесс захватывал игрока и был интересен. Таким образом, мы можем использовать глубокое обучение для создания интеллектуальных агентов с оптимальной сложностью прохождения для игрока или подбора коэффициентов в экономической системе со множеством параметров.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
